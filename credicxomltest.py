# -*- coding: utf-8 -*-
"""CredicxoMLtest.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-S80UL_brQzSJkoEE29Y3ILhLVcktFBB
"""

# Import all neccessar libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from pathlib import Path
import tensorflow as tf
from tensorflow import keras
from keras.models import Sequential
from keras.layers import Dense, Activation
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import normalize, LabelEncoder, OneHotEncoder

# Load the dataset And lets have a look at the first 5 rows
data = pd.read_csv('musk_csv.csv',index_col= 'ID')
data.head()

# Lets get some details of the dataset
data.info()

# Let us check if there is any null values in the dataset
data.isnull().sum()

# lets drop the entire row if there is any null values
data.dropna(inplace = True)

# Again lets check for details of dataset to check if there was any null values
data.info()

# No null values where found in the data
# Now we will split the Conformation Name at + & _
 new1 = data["conformation_name"].str.split("_",n = 1,expand = True)
data["conformation_i"] = new1[0]
new2 = new1[1].str.split("+",n = 1,expand = True)
data["conformation_ii"] = new2[0]
data["conformation_iii"] = new2[1]
data.drop(columns = "conformation_name", inplace = True)

# since adding new column gets attached at the end of the table , we will bring them at the beginning
col1 = data.pop('conformation_i')
col2 = data.pop('conformation_ii')
col3 = data.pop('conformation_iii')
data.insert(1, col1.name, col1)
data.insert(2, col2.name, col2)
data.insert(3, col3.name, col3)

# Lets check for the changes
data.head()

# since the molecule_name and conformation_i is same we will drop one
del data['conformation_i']
# And encode the other with LabelEncoder and OneHotEncoder
lenc = LabelEncoder()
data["molecule_name"] = lenc.fit_transform(data["molecule_name"])
ohenc = OneHotEncoder(categorical_features=[0])
data = ohenc.fit_transform(data).toarray()

# since the data is clean and also coverted to array lets have a look at the data
data

# Now we will split the data into input and output features
X = data[:,:-1]
y = data[:,-1]

# Before processing the data we will normalize the input data for better accuracy
X = normalize(X)

# Now lets split the data into training and validation set with 80:20 ratio
X_train, X_test, y_train, y_test = train_test_split(X,y,train_size = 0.80, random_state = 7, stratify = y)

print(X_train.shape)
print(X_test.shape)
print(y_train.shape)
print(y_test.shape)

# lets create our deep learning model
model = tf.keras.models.Sequential([tf.keras.layers.Dense(1024,activation = 'relu', input_dim = 270),
                                    tf.keras.layers.Dropout(0.2),
                                    tf.keras.layers.Dense(512,activation='relu'),
                                    tf.keras.layers.Dropout(0.2),
                                    tf.keras.layers.Dense(256,activation='relu'),
                                    tf.keras.layers.Dropout(0.2),
                                    tf.keras.layers.Dense(128,activation='relu'),
                                    tf.keras.layers.Dense(1,activation = 'sigmoid')])

model.summary()

# Compile the model
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

history = model.fit(X_train,y_train, validation_split=0.20, epochs=15, batch_size=16, verbose=1)

# Plot training & validation accuracy values
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('Model accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train', 'Test'], loc='upper left')
plt.show()

# Plot training & validation loss values
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Model loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Train', 'Test'], loc='upper left')
plt.show()

model.evaluate(X_test,y_test,verbose=2 )

from sklearn.metrics import classification_report

y_pred = model.predict(X_test, batch_size=16, verbose=1)
y_pred_bool = np.argmax(y_pred, axis=1)

print(classification_report(y_test, y_pred_bool))

# Now we will save neural network structure and the trained weights

model_structure = model.to_json()
f = Path("model_structure.json")    #Structure

f.write_text(model_structure)
model.save_weights("model_weights.h5")      #Trained weights